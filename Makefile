# ==============================================================================
# Makefile â€” Multimodal Hateful Memes Classifier
# ==============================================================================
# Usage:
#   make help                  Show all targets
#   make all                   Full pipeline: install â†’ ocr â†’ train â†’ eval
#   make train                 Train fusion model (default)
#   make train BATCH_SIZE=64   Train with custom hyperparameters
#   make api                   Start FastAPI server
#   make test                  Run API test suite
# ==============================================================================

.DEFAULT_GOAL := help

# ------------------------------------------------------------------------------
# Shell & flags
# ------------------------------------------------------------------------------
SHELL := /bin/bash
.SHELLFLAGS := -eu -o pipefail -c

# ------------------------------------------------------------------------------
# Directories
# ------------------------------------------------------------------------------
SRC_DIR       := src
DATA_DIR      := data
CHECKPOINT_DIR := checkpoints
RESULTS_DIR   := results
IMG_DIR       := $(DATA_DIR)/img

# ------------------------------------------------------------------------------
# Configurable hyperparameters (override on CLI)
# e.g.: make train BATCH_SIZE=64 EPOCHS=30 LR=1e-4
# ------------------------------------------------------------------------------
BATCH_SIZE  ?= 32
NUM_WORKERS ?= 4
LR          ?= 2e-4
WEIGHT_DECAY ?= 1e-4
EPOCHS      ?= 20
WARMUP      ?= 2
PATIENCE    ?= 5
DROPOUT     ?= 0.3
SEED        ?= 42
USE_OCR     ?= True
PORT        ?= 8000

# ------------------------------------------------------------------------------
# Python
# ------------------------------------------------------------------------------
PYTHON := python3
PIP    := pip3

# ------------------------------------------------------------------------------
# Sentinel files â€” prevent re-running expensive one-time steps
# ------------------------------------------------------------------------------
SENTINEL_DIR  := .sentinels
INSTALL_DONE  := $(SENTINEL_DIR)/.install.done
OCR_DONE      := $(SENTINEL_DIR)/.ocr.done
SPLITS_DONE   := $(SENTINEL_DIR)/.splits.done

# ============================================================================
# PHONY TARGETS
# ============================================================================
.PHONY: help all \
        install check-env \
        ocr splits \
        train train-fusion train-image train-text train-caption train-all \
        eval eval-fusion eval-image eval-text eval-all \
        api \
        test test-api test-unit \
        lint format type-check \
        clean clean-results clean-checkpoints clean-sentinels clean-all \
        docker-build docker-run \
        report

# ============================================================================
# HELP
# ============================================================================
help:
	@printf "\n\033[1;36mðŸ† Multimodal Hateful Memes Classifier\033[0m\n"
	@printf "\033[1;36m=======================================\033[0m\n\n"
	@printf "\033[1mUsage:\033[0m  make \033[4mtarget\033[0m [\033[4mOPTION\033[0m=\033[4mvalue\033[0m ...]\n\n"
	@printf "\033[1;33mðŸ“¦ Setup\033[0m\n"
	@printf "  %-22s %s\n" "install"          "Install all Python dependencies"
	@printf "  %-22s %s\n" "check-env"        "Validate environment (Python, CUDA, deps)"
	@printf "\n\033[1;33mðŸ”§ Data\033[0m\n"
	@printf "  %-22s %s\n" "ocr"              "Pre-compute OCR cache  [one-time, ~15 min]"
	@printf "  %-22s %s\n" "splits"           "Generate train/val/test splits"
	@printf "\n\033[1;33mðŸš€ Training\033[0m\n"
	@printf "  %-22s %s\n" "train"            "Train fusion model (default)"
	@printf "  %-22s %s\n" "train-fusion"     "Train fusion model (image + text + OCR)"
	@printf "  %-22s %s\n" "train-image"      "Train image-only baseline"
	@printf "  %-22s %s\n" "train-text"       "Train text-only baseline"
	@printf "  %-22s %s\n" "train-caption"    "Train fusion without OCR (caption only)"
	@printf "  %-22s %s\n" "train-all"        "Train all models (for full ablation)"
	@printf "\n\033[1;33mðŸ“Š Evaluation\033[0m\n"
	@printf "  %-22s %s\n" "eval"             "Evaluate fusion model on test set"
	@printf "  %-22s %s\n" "eval-all"         "Evaluate all models"
	@printf "\n\033[1;33mðŸ“¡ API\033[0m\n"
	@printf "  %-22s %s\n" "api"              "Start FastAPI server (PORT=8000)"
	@printf "\n\033[1;33mðŸ§ª Testing & Quality\033[0m\n"
	@printf "  %-22s %s\n" "test"             "Run full test suite"
	@printf "  %-22s %s\n" "test-api"         "Test live API endpoints with curl"
	@printf "  %-22s %s\n" "test-unit"        "Run unit tests with pytest"
	@printf "  %-22s %s\n" "lint"             "Lint code with flake8"
	@printf "  %-22s %s\n" "format"           "Auto-format code with black + isort"
	@printf "  %-22s %s\n" "type-check"       "Static type checking with mypy"
	@printf "\n\033[1;33mðŸ³ Docker\033[0m\n"
	@printf "  %-22s %s\n" "docker-build"     "Build Docker image"
	@printf "  %-22s %s\n" "docker-run"       "Run API in Docker container"
	@printf "\n\033[1;33mðŸ§¹ Cleanup\033[0m\n"
	@printf "  %-22s %s\n" "clean"            "Remove cache and .pyc files"
	@printf "  %-22s %s\n" "clean-results"    "Remove results/ (keep checkpoints)"
	@printf "  %-22s %s\n" "clean-checkpoints" "Remove checkpoints/"
	@printf "  %-22s %s\n" "clean-all"        "Remove everything (incl. data artifacts)"
	@printf "\n\033[1;33mâš¡ Pipelines\033[0m\n"
	@printf "  %-22s %s\n" "all"              "install â†’ ocr â†’ splits â†’ train â†’ eval"
	@printf "  %-22s %s\n" "report"           "Generate final evaluation report"
	@printf "\n\033[1mConfigurable options (with defaults):\033[0m\n"
	@printf "  %-22s %s\n" "BATCH_SIZE=32"    "Mini-batch size"
	@printf "  %-22s %s\n" "EPOCHS=20"        "Maximum training epochs"
	@printf "  %-22s %s\n" "LR=2e-4"          "Learning rate"
	@printf "  %-22s %s\n" "WEIGHT_DECAY=1e-4" "AdamW weight decay"
	@printf "  %-22s %s\n" "PATIENCE=5"       "Early stopping patience"
	@printf "  %-22s %s\n" "DROPOUT=0.3"      "Fusion MLP dropout rate"
	@printf "  %-22s %s\n" "SEED=42"          "Global random seed"
	@printf "  %-22s %s\n" "USE_OCR=True"     "Include OCR text in text modality"
	@printf "  %-22s %s\n" "PORT=8000"        "FastAPI port"
	@printf "\n\033[1mExamples:\033[0m\n"
	@printf "  make train BATCH_SIZE=64 EPOCHS=30 LR=1e-4\n"
	@printf "  make train-all && make eval-all\n"
	@printf "  make api PORT=9000 &\n"
	@printf "  make test-api\n\n"

# ============================================================================
# SETUP
# ============================================================================
$(SENTINEL_DIR):
	@mkdir -p $(SENTINEL_DIR)

install: $(SENTINEL_DIR) requirements.txt
	@printf "\033[1;34mðŸ“¦ Installing dependencies...\033[0m\n"
	$(PIP) install --upgrade pip
	$(PIP) install -r requirements.txt
	@touch $(INSTALL_DONE)
	@printf "\033[1;32mâœ… Dependencies installed\033[0m\n"

check-env:
	@printf "\033[1;34mðŸ” Checking environment...\033[0m\n"
	@$(PYTHON) --version
	@$(PYTHON) -c "import torch; print(f'PyTorch: {torch.__version__}')"
	@$(PYTHON) -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
	@$(PYTHON) -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\"}')"
	@$(PYTHON) -c "import transformers; print(f'Transformers: {transformers.__version__}')"
	@$(PYTHON) -c "import fastapi; print(f'FastAPI: {fastapi.__version__}')"
	@printf "\033[1;32mâœ… Environment OK\033[0m\n"

# ============================================================================
# DATA
# ============================================================================
$(OCR_DONE): $(SENTINEL_DIR)
	@printf "\033[1;34mðŸ”§ Pre-computing OCR cache...\033[0m\n"
	$(PYTHON) -m $(SRC_DIR).precompute_ocr
	@touch $(OCR_DONE)
	@printf "\033[1;32mâœ… OCR cache ready: $(DATA_DIR)/ocr_cache.json\033[0m\n"

ocr: $(OCR_DONE)

$(SPLITS_DONE): $(SENTINEL_DIR)
	@printf "\033[1;34mðŸ”€ Generating train/val/test splits...\033[0m\n"
	$(PYTHON) -c "from $(SRC_DIR).data import create_splits; create_splits(seed=$(SEED))"
	@touch $(SPLITS_DONE)
	@printf "\033[1;32mâœ… Splits saved: $(DATA_DIR)/splits.json\033[0m\n"

splits: $(SPLITS_DONE)

# ============================================================================
# TRAINING â€” shared Python inline runner
# ============================================================================
define run_train
	$(PYTHON) -c "\
import sys; sys.path.insert(0, '.'); \
from $(SRC_DIR).train import CONFIG, main; \
CONFIG.update({ \
    'model_type':   '$(1)', \
    'use_ocr':      $(2), \
    'batch_size':   $(BATCH_SIZE), \
    'num_workers':  $(NUM_WORKERS), \
    'lr':           $(LR), \
    'weight_decay': $(WEIGHT_DECAY), \
    'epochs':       $(EPOCHS), \
    'warmup':       $(WARMUP), \
    'patience':     $(PATIENCE), \
    'dropout':      $(DROPOUT), \
    'seed':         $(SEED), \
}); main()"
endef

train: train-fusion

train-fusion: ocr splits
	@printf "\033[1;34mðŸš€ Training: fusion (image + text + OCR)\033[0m\n"
	@printf "   batch=$(BATCH_SIZE) | lr=$(LR) | epochs=$(EPOCHS) | seed=$(SEED)\n"
	@mkdir -p $(CHECKPOINT_DIR) $(RESULTS_DIR)
	$(call run_train,fusion,$(USE_OCR))
	@printf "\033[1;32mâœ… Fusion model trained â†’ $(CHECKPOINT_DIR)/best_fusion.pt\033[0m\n"

train-image: splits
	@printf "\033[1;34mðŸš€ Training: image-only baseline\033[0m\n"
	@mkdir -p $(CHECKPOINT_DIR) $(RESULTS_DIR)
	$(call run_train,image,False)
	@printf "\033[1;32mâœ… Image model trained â†’ $(CHECKPOINT_DIR)/best_image.pt\033[0m\n"

train-text: splits
	@printf "\033[1;34mðŸš€ Training: text-only baseline\033[0m\n"
	@mkdir -p $(CHECKPOINT_DIR) $(RESULTS_DIR)
	$(call run_train,text,False)
	@printf "\033[1;32mâœ… Text model trained â†’ $(CHECKPOINT_DIR)/best_text.pt\033[0m\n"

train-caption: splits
	@printf "\033[1;34mðŸš€ Training: fusion without OCR (caption only)\033[0m\n"
	@mkdir -p $(CHECKPOINT_DIR) $(RESULTS_DIR)
	$(call run_train,fusion,False)
	@printf "\033[1;32mâœ… Caption-fusion model trained\033[0m\n"

train-all: train-fusion train-image train-text train-caption
	@printf "\033[1;32mâœ… All models trained\033[0m\n"

# ============================================================================
# EVALUATION
# ============================================================================
define run_eval
	$(PYTHON) -c "\
import sys; sys.path.insert(0, '.'); \
from $(SRC_DIR).eval import CONFIG, main; \
CONFIG.update({'model_type': '$(1)', 'use_ocr': $(2)}); main()"
endef

eval: eval-fusion

eval-fusion:
	@printf "\033[1;34mðŸ“Š Evaluating: fusion model\033[0m\n"
	@test -f $(CHECKPOINT_DIR)/best_fusion.pt || \
		(printf "\033[1;31mâŒ Checkpoint not found. Run: make train-fusion\033[0m\n" && exit 1)
	$(call run_eval,fusion,True)
	@printf "\033[1;32mâœ… Results â†’ $(RESULTS_DIR)/metrics_fusion.json\033[0m\n"

eval-image:
	@printf "\033[1;34mðŸ“Š Evaluating: image-only model\033[0m\n"
	@test -f $(CHECKPOINT_DIR)/best_image.pt || \
		(printf "\033[1;31mâŒ Checkpoint not found. Run: make train-image\033[0m\n" && exit 1)
	$(call run_eval,image,False)
	@printf "\033[1;32mâœ… Results â†’ $(RESULTS_DIR)/metrics_image.json\033[0m\n"

eval-text:
	@printf "\033[1;34mðŸ“Š Evaluating: text-only model\033[0m\n"
	@test -f $(CHECKPOINT_DIR)/best_text.pt || \
		(printf "\033[1;31mâŒ Checkpoint not found. Run: make train-text\033[0m\n" && exit 1)
	$(call run_eval,text,False)
	@printf "\033[1;32mâœ… Results â†’ $(RESULTS_DIR)/metrics_text.json\033[0m\n"

eval-all: eval-fusion eval-image eval-text
	@printf "\n\033[1;32mðŸ“Š All evaluation results:\033[0m\n"
	@for f in $(RESULTS_DIR)/metrics_*.json; do \
		printf "  %-40s " "$$f"; \
		$(PYTHON) -c "import json,sys; d=json.load(open('$$f')); print(f\"Acc={d.get('accuracy',0)*100:.2f}%  AUC={d.get('roc_auc',0)*100:.2f}%\")"; \
	done

# ============================================================================
# API
# ============================================================================
api:
	@printf "\033[1;34mðŸ“¡ Starting FastAPI server on http://localhost:$(PORT)\033[0m\n"
	@printf "   Endpoints: GET / /health /info | POST /predict\n"
	@printf "   Press Ctrl+C to stop\n\n"
	$(PYTHON) app.py --port $(PORT)

# ============================================================================
# TESTING & CODE QUALITY
# ============================================================================
test: test-unit test-api

test-unit:
	@printf "\033[1;34mðŸ§ª Running unit tests...\033[0m\n"
	$(PYTHON) -m pytest tests/ -v --tb=short 2>/dev/null || \
		printf "\033[1;33mâš ï¸  No tests/ directory found â€” skipping unit tests\033[0m\n"

test-api:
	@printf "\033[1;34mðŸ§ª Testing API endpoints...\033[0m\n"
	@printf "\n[1/4] Health check\n"
	@curl -sf http://localhost:$(PORT)/health | $(PYTHON) -m json.tool || \
		(printf "\033[1;31mâŒ Server not running. Start with: make api\033[0m\n" && exit 1)
	@printf "\n[2/4] Root endpoint\n"
	@curl -sf http://localhost:$(PORT)/ | $(PYTHON) -m json.tool
	@printf "\n[3/4] Model info\n"
	@curl -sf http://localhost:$(PORT)/info | $(PYTHON) -m json.tool
	@printf "\n[4/4] Predict (sample image)\n"
	@SAMPLE=$$(ls $(IMG_DIR)/*.png 2>/dev/null | head -1); \
	if [ -n "$$SAMPLE" ]; then \
		curl -sf -X POST "http://localhost:$(PORT)/predict" \
			-F "image=@$$SAMPLE" \
			-F "caption=test caption" | $(PYTHON) -m json.tool; \
	else \
		printf "\033[1;33mâš ï¸  No sample images found in $(IMG_DIR)/\033[0m\n"; \
	fi
	@printf "\n\033[1;32mâœ… API test suite passed\033[0m\n"

lint:
	@printf "\033[1;34mðŸ” Linting with flake8...\033[0m\n"
	$(PYTHON) -m flake8 $(SRC_DIR)/ app.py \
		--max-line-length=100 \
		--ignore=E203,W503 \
		--exclude=__pycache__
	@printf "\033[1;32mâœ… Lint passed\033[0m\n"

format:
	@printf "\033[1;34mðŸŽ¨ Formatting with black + isort...\033[0m\n"
	$(PYTHON) -m isort $(SRC_DIR)/ app.py
	$(PYTHON) -m black $(SRC_DIR)/ app.py --line-length=100
	@printf "\033[1;32mâœ… Formatting complete\033[0m\n"

type-check:
	@printf "\033[1;34mðŸ”Ž Type checking with mypy...\033[0m\n"
	$(PYTHON) -m mypy $(SRC_DIR)/ app.py \
		--ignore-missing-imports \
		--no-strict-optional
	@printf "\033[1;32mâœ… Type check passed\033[0m\n"


# ============================================================================
# REPORT
# ============================================================================
report:
	@printf "\033[1;34mðŸ“ Generating evaluation report...\033[0m\n"
	@test -f $(RESULTS_DIR)/metrics_fusion.json || \
		(printf "\033[1;31mâŒ Run 'make eval' first\033[0m\n" && exit 1)
	$(PYTHON) -c "\
import json, datetime; \
m = json.load(open('$(RESULTS_DIR)/metrics_fusion.json')); \
print(f\"\"\"# Evaluation Summary â€” {datetime.date.today()}\n\
| Metric    | Value |\n|-----------|---------|\n\
| Accuracy  | {m.get('accuracy',0)*100:.2f}% |\n\
| Precision | {m.get('precision',0)*100:.2f}% |\n\
| Recall    | {m.get('recall',0)*100:.2f}% |\n\
| F1        | {m.get('f1',0)*100:.2f}% |\n\
| ROC-AUC   | {m.get('roc_auc',0)*100:.2f}% |\n\"\"\")"
	@printf "\033[1;32mâœ… Report printed above\033[0m\n"

# ============================================================================
# CLEANUP
# ============================================================================
clean:
	@printf "\033[1;34mðŸ§¹ Cleaning cache and bytecode...\033[0m\n"
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	find . -type f -name "*.pyc" -delete 2>/dev/null || true
	find . -type f -name "*.pyo" -delete 2>/dev/null || true
	find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	find . -type d -name ".mypy_cache" -exec rm -rf {} + 2>/dev/null || true
	@printf "\033[1;32mâœ… Clean complete (data/ and checkpoints/ preserved)\033[0m\n"

clean-results:
	@printf "\033[1;34mðŸ§¹ Removing results/...\033[0m\n"
	rm -rf $(RESULTS_DIR)
	@printf "\033[1;32mâœ… Results removed\033[0m\n"

clean-checkpoints:
	@printf "\033[1;34mðŸ§¹ Removing checkpoints/...\033[0m\n"
	rm -rf $(CHECKPOINT_DIR)
	@printf "\033[1;32mâœ… Checkpoints removed\033[0m\n"

clean-sentinels:
	@rm -rf $(SENTINEL_DIR)

clean-all: clean clean-results clean-checkpoints clean-sentinels
	@printf "\033[1;34mðŸ§¹ Removing data artifacts...\033[0m\n"
	rm -f $(DATA_DIR)/splits.json $(DATA_DIR)/ocr_cache.json
	@printf "\033[1;32mâœ… Full clean complete\033[0m\n"

# ============================================================================
# ONE-COMMAND PIPELINE
# ============================================================================
all: install ocr splits train-fusion eval-fusion
	@printf "\n\033[1;32mðŸŽ‰ Full pipeline complete!\033[0m\n"
	@printf "   âœ… Dependencies installed\n"
	@printf "   âœ… OCR cache generated\n"
	@printf "   âœ… Splits created\n"
	@printf "   âœ… Fusion model trained\n"
	@printf "   âœ… Evaluation complete\n"
	@printf "\n   ðŸ“Š Results:  $(RESULTS_DIR)/metrics_fusion.json\n"
	@printf "   ðŸ¤– Model:    $(CHECKPOINT_DIR)/best_fusion.pt\n"
	@printf "   ðŸ“¡ API:      make api\n\n"
